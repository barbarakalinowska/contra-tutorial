{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python in Life Sciences\n",
    "\n",
    "\n",
    "The aim o the tutorial is to show usage of the main Python tools used in bioinformatics contexts and to present a workflow of preparing a simple CLI application.\n",
    "\n",
    "\n",
    "Outline:\n",
    "1. Prototyping\n",
    "    * processing SAM files (pysam)\n",
    "    * processing TSV files (pandas, pyarrow)\n",
    "    * adding multiprocessing \n",
    "2. CLI application counting gRNAs from alignment files\n",
    "    \n",
    "Problem:\n",
    "Let's assume we have a data from the CRISPR screen experimemt. For the purpose of the tutorial we are going to focus on one step of the data processing - counting genes indentified by guide RNA sequences aligned to the library. The real analysis requires to start with demultiplexing, trimming the reads and performing and alignment to a library. Let's assume we have these steps performed already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: process the SAM file in order to get counts of genes indentified by the gRNAs. Write results to a TSV file and visualize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing SAM files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: parse SAM file with pysam module read by read.\n",
    "\n",
    "Firstly, let's import some basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pysam\n",
    "import seaborn as sns\n",
    "\n",
    "from Bio import SeqIO\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming someone would like to parse the library in order to obtain all gRNAs identifiers, the easiest way is to parse FASTA with with the library with the Biopython module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"data/screen_library.fasta\"\n",
    "\n",
    "genes_count = dict()\n",
    "with open(library, \"r\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        genes_count[record.id.split(\"_\")[0]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the SAM file and print some information about each read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = pysam.AlignmentFile(\"data/sam_files/sample1.sam\", \"r\")\n",
    "for read in sam.fetch():\n",
    "    print(read.reference_name, read.reference_name, read.is_unmapped)\n",
    "    print(read.query_name, read.query_sequence, read.query_length, read.get_tags())\n",
    "sam.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a dictionary of all genes, we go through th SAM file and count the genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_file = \"data/sam_files/sample1.sam\"\n",
    "sam = pysam.AlignmentFile(sam_file, \"r\")\n",
    "for read in sam.fetch():\n",
    "    if not read.is_unmapped and read.qlen <= 20 and read.get_tag('NM') == 1:\n",
    "        genes_count[read.reference_name.split(\"_\", 1)[0]] += 1\n",
    "sam.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the results in a TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_report = 'report_test.tsv'\n",
    "header = [\"gene\", \"count\"]\n",
    "with open(counts_report, 'w') as csvfile:\n",
    "    gene_count_csv = csv.writer(csvfile, delimiter='\\t')\n",
    "    gene_count_csv.writerow(header)\n",
    "    for gene in sorted(genes_count.keys()):\n",
    "        gene_count_csv.writerow([gene, genes_count[gene]])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather all the steps into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report(genes_count, report):\n",
    "    header = [\"gene\", \"count\"]\n",
    "    with open(report, 'w') as csvfile:\n",
    "        gene_count_csv = csv.writer(csvfile, delimiter='\\t')\n",
    "        gene_count_csv.writerow(header)\n",
    "        for gene in sorted(genes_count.keys()):\n",
    "            gene_count_csv.writerow([gene, genes_count[gene]])  \n",
    "\n",
    "            \n",
    "def count_genes_pysam(sam_aln, report):\n",
    "    sam = pysam.AlignmentFile(sam_aln, \"rb\")\n",
    "    genes_count = defaultdict(int)\n",
    "    for read in sam.fetch():\n",
    "        if not read.is_unmapped and read.qlen <= 20 and read.get_tag('NM') == 1:\n",
    "            genes_count[read.reference_name.split(\"_\", 1)[0]] += 1\n",
    "    sam.close()\n",
    "    write_report(genes_count, report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the function's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit count_genes_pysam(\"data/sam_files/sample1.sam\", \"counts_report1.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing TSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: treat the SAM file as a TSV file.\n",
    "\n",
    "We are going to use pandas to process the TSV file quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the SAM file do not have a constant number of columns, we need to define the columns before we read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"read_id\", \"flags_sum\", \"ref\", \"pos\", \"quality\", \"cigar\", \n",
    "        \"ref_aln\", \"aln_pos\", \"insert\", \"read_seq\", \"aln\", \n",
    "        \"opt1\", \"opt2\", \"opt3\", \"opt4\", \"opt5\", \"opt6\", \"opt7\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln = pd.read_csv(\"data/sam_files/sample1.sam\", delimiter=\"\\t\", names=column_names, comment=\"@\", \n",
    "                  index_col=False, compression='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln.to_csv(\"data/tsv_files/sample1.tsv.gz\", sep=\"\\t\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly investigate the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to keep only aligned reads, to we can filter out the unmapped reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln['cigar'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unmapped reads should be removed, so filter out the rows with '\\*' in the 'cigar' field: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_filtered = aln[aln['cigar'] != \"*\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_filtered['gene'] = aln_filtered['ref'].str.split(\"_\", 1, expand=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use aggregation function to count occurence of genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_count = aln_filtered.groupby('gene').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_count.to_csv(\"counts_report2.tsv\", sep=\"\\t\", header=[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the processing will be now gathered as a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_genes_pandas(sam_aln, report, cols):\n",
    "    aln = pd.read_csv(sam_aln, delimiter=\"\\t\", names=cols, comment=\"@\", \n",
    "                  index_col=False, compression='infer')\n",
    "    aln = aln[aln['cigar'] != \"*\"]\n",
    "    aln['gene'] = aln['ref'].str.split(\"_\", 1, expand=True)[0]\n",
    "    genes_count = aln.groupby('gene').size()\n",
    "    genes_count.to_csv(report, sep=\"\\t\", header=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time count_genes_pandas(\"data/sam_files/sample1.sam\", \"count_report_pd.tsv\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time count_genes_pandas(\"data/tsv_files/sample1.tsv.gz\", \"count_report_pd.tsv\", column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet files\n",
    "\n",
    "#### Approach 3: converting the file into parquet files.\n",
    "In this approach we are testing what may be a benefit of storing the data in a parquet files instead of TSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_file = pd.read_table('data/sam_files/sample1.sam', comment=\"@\", \n",
    "                       names=column_names, index_col=False, compression='infer')\n",
    "pq.write_table(pa.Table.from_pandas(pd_file), 'data/sample1.pq', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_genes_pq(pq_file, report):\n",
    "    aln = pd.read_parquet(pq_file, use_threads=True)\n",
    "    aln = aln[aln['cigar'] != \"*\"]\n",
    "    aln['gene'] = aln['ref'].str.split(\"_\", 1, expand=True)[0]\n",
    "    genes_count = aln.groupby('gene').size().to_frame(name=\"count\")\n",
    "    genes_count.to_csv(report, sep=\"\\t\", header=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time count_genes_pq(\"data/sample1.pq\", \"data/count_report.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 4: dividing a file into chunks\n",
    "\n",
    "This approach may be especially useful if large files will be processed and when someone would like to avoid reading the whole file into memory.  Also, if someone would like to process the large file and keep intermediate results.\n",
    "In this approach we are going to use pyarrow module, which allows us to write data into parquet files. One of very important advantage of this operation is improving the peformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_table('data/sam_files/sample1.sam', chunksize=1e3, comment=\"@\", \n",
    "                       names=column_names, index_col=False, compression='infer')\n",
    "\n",
    "for chunk_no, chunk in enumerate(reader):\n",
    "    pq.write_table(pa.Table.from_pandas(chunk),\n",
    "    os.path.join('data/pq_files_in', 'aln-{:04d}.parquet'.format(chunk_no)), compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genes(df):\n",
    "    df = df[df['cigar'] != \"*\"]\n",
    "    df['gene'] = df['ref'].str.split(\"_\", 1, expand=True)[0]\n",
    "    return df[['gene']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aln(filename, out_dir):\n",
    "    chunk = pq.read_table(filename, use_threads=True).to_pandas()\n",
    "    chunk_genes = extract_genes(chunk)\n",
    "    pq.write_table(pa.Table.from_pandas(chunk_genes), \n",
    "                   os.path.join(out_dir, os.path.basename(filename)), \n",
    "                   compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pool = mp.Pool()\n",
    "out_dir = \"data/pq_files_out\"\n",
    "in_dir = \"data/pq_files_in\"\n",
    "for filename in glob.glob(os.path.join(in_dir, '*.parquet')):\n",
    "    pool.apply_async(process_aln, args=(filename, out_dir, ))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pq.read_table('data/pq_files_out/', use_threads=True).to_pandas()\n",
    "genes = df.groupby('gene').size().to_frame(name=\"count\")\n",
    "genes.to_csv('counts_report_pq.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool = mp.Pool()\n",
    "out_dir = \"data/pq_files_out\"\n",
    "in_dir = \"data/pq_files_in\"\n",
    "for filename in glob.glob(os.path.join(in_dir, '*.parquet')):\n",
    "    pool.apply_async(process_aln, args=(filename, out_dir, ))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "df = pq.read_table('data/pq_files_out/', use_threads=True).to_pandas()\n",
    "genes = df.groupby('gene').size().to_frame(name=\"count\")\n",
    "genes.to_csv('counts_report_pq.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to process all four samples and then the results will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_dir = \"./data/sam_files/\"\n",
    "sam_files = glob.glob(os.path.join(sam_dir, '*.sam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sam_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_genes_pandas(sam_aln, report, cols):\n",
    "    aln = pd.read_csv(sam_aln, delimiter=\"\\t\", names=cols, comment=\"@\", \n",
    "                  index_col=False, compression='infer')\n",
    "    aln = aln[aln['cigar'] != \"*\"]\n",
    "    aln['gene'] = aln['ref'].str.split(\"_\", 1, expand=True)[0]\n",
    "    genes_count = aln.groupby('gene').size()\n",
    "    genes_count.to_csv(report, sep=\"\\t\", header=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sam_file in sam_files:\n",
    "    sample = os.path.basename(sam_file).split(\".\")[0]\n",
    "    count_genes_pandas(sam_file, sample+\"_report.tsv\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_counts_1 = pd.read_csv(\"data/tsv_files/sample1_report.tsv\", sep=\"\\t\")\n",
    "gene_counts_1 = gene_counts_1.set_index('gene')\n",
    "gene_counts_2 = pd.read_csv(\"data/tsv_files/sample2_report.tsv\", sep=\"\\t\")\n",
    "gene_counts_2 = gene_counts_2.set_index(\"gene\")\n",
    "gene_counts_3 = pd.read_csv(\"data/tsv_files/sample3_report.tsv\", sep=\"\\t\")\n",
    "gene_counts_3 = gene_counts_3.set_index(\"gene\")\n",
    "gene_counts_4 = pd.read_csv(\"data/tsv_files/sample4_report.tsv\", sep=\"\\t\")\n",
    "gene_counts_4 = gene_counts_4.set_index(\"gene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_counts_all =  pd.merge(gene_counts_1, gene_counts_2,\n",
    "                            on=\"gene\", how=\"outer\", suffixes=['_1', '_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_counts_all_p2 =  pd.merge(gene_counts_3, gene_counts_4,\n",
    "                            on=\"gene\", how=\"outer\", suffixes=['_3', '_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_counts_all = pd.merge(gene_counts_all, gene_counts_all_p2, on=\"gene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_counts_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_counts_all.columns = [\"sample1\", \"sample2\", \"sample3\", \"sample4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "data = pd.melt(gene_counts_all)\n",
    "ax = sns.boxplot(x=\"variable\", y=\"value\", data=data)\n",
    "ax.set(xlabel='sample', ylabel='counts', title=\"Genes abundance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in gene_counts_all.columns:\n",
    "#     ax = sns.distplot(gene_counts_all[sample].dropna(), kde=True, kde_kws = {'shade': True, 'linewidth': 3}, \n",
    "#                       hist=False, label=sample)\n",
    "    ax = sns.distplot(gene_counts_all[sample].dropna(), kde=False, hist=True, label=sample)\n",
    "ax.set(xlabel='counts', title=\"Genes abundance\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
